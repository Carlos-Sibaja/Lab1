{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419778cd-43cd-4e67-af2e-507a614cdff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# For machine learning\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2ca2f5-d772-406b-ad62-3c1a6c787913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 619040 entries, 0 to 619039\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    619040 non-null  object \n",
      " 1   open    619029 non-null  float64\n",
      " 2   high    619032 non-null  float64\n",
      " 3   low     619032 non-null  float64\n",
      " 4   close   619040 non-null  float64\n",
      " 5   volume  619040 non-null  int64  \n",
      " 6   name    619040 non-null  object \n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset to understand its structure\n",
    "csv_filename= pd.read_csv('all_stocks_5yr.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "csv_filename.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77092525-0494-4e29-ad6d-2ab8597fb894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CSV vs. Parquet Benchmark Results ===\n",
      "    Scale  CSV_Size_MB  CSV_Write_Time_s  CSV_Read_Time_s  Parquet_Size_MB  \\\n",
      "0       1    28.800573          1.681946         0.285367        12.730873   \n",
      "1       1    28.800573          1.268893         0.270100        10.151073   \n",
      "2       1    28.800573          1.341022         0.294221         8.058299   \n",
      "3       1    28.800573          1.413980         0.334922         7.755613   \n",
      "4      10   288.005407         13.368246         2.418940       118.028409   \n",
      "5      10   288.005407         15.014923         2.515728        95.354448   \n",
      "6      10   288.005407         14.243311         2.842862        75.976962   \n",
      "7      10   288.005407         13.672949         6.751509        73.180475   \n",
      "8     100  2880.053747        152.582843        25.106432      1178.436979   \n",
      "9     100  2880.053747        140.351567        26.087857       951.709143   \n",
      "10    100  2880.053747        152.033769        26.132726       758.129578   \n",
      "11    100  2880.053747        142.250627        23.911745       730.511627   \n",
      "\n",
      "    Parquet_Write_Time_s  Parquet_Read_Time_s Compression  \n",
      "0               0.254456             0.095066        None  \n",
      "1               0.275099             0.120534      snappy  \n",
      "2               0.882644             0.087183        gzip  \n",
      "3               0.924692             0.077551      brotli  \n",
      "4               2.271214             0.682673        None  \n",
      "5               2.134055             0.777165      snappy  \n",
      "6               7.791368             1.179863        gzip  \n",
      "7               9.350809             1.480582      brotli  \n",
      "8              21.250852             8.634975        None  \n",
      "9              24.474314             8.305940      snappy  \n",
      "10             83.451986             9.755618        gzip  \n",
      "11             80.562083             5.895207      brotli  \n",
      "\n",
      "=== CSV vs. Parquet Benchmark Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scale</th>\n",
       "      <th>CSV_Size_MB</th>\n",
       "      <th>CSV_Write_Time_s</th>\n",
       "      <th>CSV_Read_Time_s</th>\n",
       "      <th>Parquet_Size_MB</th>\n",
       "      <th>Parquet_Write_Time_s</th>\n",
       "      <th>Parquet_Read_Time_s</th>\n",
       "      <th>Compression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>28.800573</td>\n",
       "      <td>1.681946</td>\n",
       "      <td>0.285367</td>\n",
       "      <td>12.730873</td>\n",
       "      <td>0.254456</td>\n",
       "      <td>0.095066</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28.800573</td>\n",
       "      <td>1.268893</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>10.151073</td>\n",
       "      <td>0.275099</td>\n",
       "      <td>0.120534</td>\n",
       "      <td>snappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>28.800573</td>\n",
       "      <td>1.341022</td>\n",
       "      <td>0.294221</td>\n",
       "      <td>8.058299</td>\n",
       "      <td>0.882644</td>\n",
       "      <td>0.087183</td>\n",
       "      <td>gzip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28.800573</td>\n",
       "      <td>1.413980</td>\n",
       "      <td>0.334922</td>\n",
       "      <td>7.755613</td>\n",
       "      <td>0.924692</td>\n",
       "      <td>0.077551</td>\n",
       "      <td>brotli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>288.005407</td>\n",
       "      <td>13.368246</td>\n",
       "      <td>2.418940</td>\n",
       "      <td>118.028409</td>\n",
       "      <td>2.271214</td>\n",
       "      <td>0.682673</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>288.005407</td>\n",
       "      <td>15.014923</td>\n",
       "      <td>2.515728</td>\n",
       "      <td>95.354448</td>\n",
       "      <td>2.134055</td>\n",
       "      <td>0.777165</td>\n",
       "      <td>snappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>288.005407</td>\n",
       "      <td>14.243311</td>\n",
       "      <td>2.842862</td>\n",
       "      <td>75.976962</td>\n",
       "      <td>7.791368</td>\n",
       "      <td>1.179863</td>\n",
       "      <td>gzip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>288.005407</td>\n",
       "      <td>13.672949</td>\n",
       "      <td>6.751509</td>\n",
       "      <td>73.180475</td>\n",
       "      <td>9.350809</td>\n",
       "      <td>1.480582</td>\n",
       "      <td>brotli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>2880.053747</td>\n",
       "      <td>152.582843</td>\n",
       "      <td>25.106432</td>\n",
       "      <td>1178.436979</td>\n",
       "      <td>21.250852</td>\n",
       "      <td>8.634975</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>2880.053747</td>\n",
       "      <td>140.351567</td>\n",
       "      <td>26.087857</td>\n",
       "      <td>951.709143</td>\n",
       "      <td>24.474314</td>\n",
       "      <td>8.305940</td>\n",
       "      <td>snappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>2880.053747</td>\n",
       "      <td>152.033769</td>\n",
       "      <td>26.132726</td>\n",
       "      <td>758.129578</td>\n",
       "      <td>83.451986</td>\n",
       "      <td>9.755618</td>\n",
       "      <td>gzip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>2880.053747</td>\n",
       "      <td>142.250627</td>\n",
       "      <td>23.911745</td>\n",
       "      <td>730.511627</td>\n",
       "      <td>80.562083</td>\n",
       "      <td>5.895207</td>\n",
       "      <td>brotli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Scale  CSV_Size_MB  CSV_Write_Time_s  CSV_Read_Time_s  Parquet_Size_MB  \\\n",
       "0       1    28.800573          1.681946         0.285367        12.730873   \n",
       "1       1    28.800573          1.268893         0.270100        10.151073   \n",
       "2       1    28.800573          1.341022         0.294221         8.058299   \n",
       "3       1    28.800573          1.413980         0.334922         7.755613   \n",
       "4      10   288.005407         13.368246         2.418940       118.028409   \n",
       "5      10   288.005407         15.014923         2.515728        95.354448   \n",
       "6      10   288.005407         14.243311         2.842862        75.976962   \n",
       "7      10   288.005407         13.672949         6.751509        73.180475   \n",
       "8     100  2880.053747        152.582843        25.106432      1178.436979   \n",
       "9     100  2880.053747        140.351567        26.087857       951.709143   \n",
       "10    100  2880.053747        152.033769        26.132726       758.129578   \n",
       "11    100  2880.053747        142.250627        23.911745       730.511627   \n",
       "\n",
       "    Parquet_Write_Time_s  Parquet_Read_Time_s Compression  \n",
       "0               0.254456             0.095066        None  \n",
       "1               0.275099             0.120534      snappy  \n",
       "2               0.882644             0.087183        gzip  \n",
       "3               0.924692             0.077551      brotli  \n",
       "4               2.271214             0.682673        None  \n",
       "5               2.134055             0.777165      snappy  \n",
       "6               7.791368             1.179863        gzip  \n",
       "7               9.350809             1.480582      brotli  \n",
       "8              21.250852             8.634975        None  \n",
       "9              24.474314             8.305940      snappy  \n",
       "10             83.451986             9.755618        gzip  \n",
       "11             80.562083             5.895207      brotli  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load the original dataset\n",
    "datastocks = \"all_stocks_5yr.csv\"  # Replace with the actual filename\n",
    "df_original = pd.read_csv(datastocks)\n",
    "\n",
    "# Benchmark function for evaluating CSV vs. Parquet\n",
    "def benchmark(df, scale_factor, compression=None):\n",
    "    \"\"\"Evaluates CSV vs. Parquet in terms of size and speed at different scales.\"\"\"\n",
    "    scaled_df = pd.concat([df] * scale_factor, ignore_index=True)\n",
    "    csv_filename = f\"data_{scale_factor}x.csv\"\n",
    "    parquet_filename = f\"data_{scale_factor}x.parquet\"\n",
    "\n",
    "    # ---- CSV Evaluation ----\n",
    "    start_time = time.time()\n",
    "    scaled_df.to_csv(csv_filename, index=False)\n",
    "    csv_write_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    df_csv = pd.read_csv(csv_filename)\n",
    "    csv_read_time = time.time() - start_time\n",
    "\n",
    "    csv_size = os.path.getsize(csv_filename) / (1024 * 1024)  # Size in MB\n",
    "\n",
    "    # ---- Parquet Evaluation ----\n",
    "    start_time = time.time()\n",
    "    scaled_df.to_parquet(parquet_filename, index=False, compression=compression, engine=\"pyarrow\")\n",
    "    parquet_write_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    df_parquet = pd.read_parquet(parquet_filename, engine=\"pyarrow\")\n",
    "    parquet_read_time = time.time() - start_time\n",
    "\n",
    "    parquet_size = os.path.getsize(parquet_filename) / (1024 * 1024)  # Size in MB\n",
    "\n",
    "    # Remove files to save space\n",
    "    os.remove(csv_filename)\n",
    "    os.remove(parquet_filename)\n",
    "\n",
    "    return {\n",
    "        \"Scale\": scale_factor,\n",
    "        \"CSV_Size_MB\": csv_size,\n",
    "        \"CSV_Write_Time_s\": csv_write_time,\n",
    "        \"CSV_Read_Time_s\": csv_read_time,\n",
    "        \"Parquet_Size_MB\": parquet_size,\n",
    "        \"Parquet_Write_Time_s\": parquet_write_time,\n",
    "        \"Parquet_Read_Time_s\": parquet_read_time,\n",
    "        \"Compression\": compression\n",
    "    }\n",
    "\n",
    "# Run benchmarks at scales 1x, 10x, 100x with different compression methods\n",
    "scales = [1, 10, 100]\n",
    "compressions = [None, \"snappy\", \"gzip\", \"brotli\"]\n",
    "results = []\n",
    "\n",
    "for scale in scales:\n",
    "    for comp in compressions:\n",
    "        results.append(benchmark(df_original, scale, comp))\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results to the console\n",
    "print(\"\\n=== CSV vs. Parquet Benchmark Results ===\")\n",
    "print(results_df)\n",
    "\n",
    "# Print the results in the Jupyter Notebook\n",
    "print(\"\\n=== CSV vs. Parquet Benchmark Results ===\")\n",
    "display(results_df)  # Display table in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34ad46-9e94-4e5e-bc23-bfd317664c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cae11951-da73-4e64-8ca3-1a1412a9ec2b",
   "metadata": {},
   "source": [
    "## First Result\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"1x.jpg\" alt=\"first result\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ae483-6e23-467f-a4ee-af09789edbbd",
   "metadata": {},
   "source": [
    "### PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3fe65d-5619-4e30-9c7d-48799ccaf1fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import polars as pl\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "from IPython.display import display\r\n",
    "\r\n",
    "# Load Dataset in both Pandas and Polars\r\n",
    "csv_file = \"all_stocks_5yr.csv\"\r\n",
    "\r\n",
    "# Load with Pandas\r\n",
    "df_pandas = pd.read_csv(csv_file)\r\n",
    "df_pandas['date'] = pd.to_datetime(df_pandas['date'])\r\n",
    "df_pandas = df_pandas.sort_values('date')\r\n",
    "\r\n",
    "# Load with Polars\r\n",
    "df_polars = pl.read_csv(csv_file)\r\n",
    "df_polars = df_polars.with_columns(pl.col(\"date\").str.to_date())\r\n",
    "\r\n",
    "print(\"Dataset successfully loaded in both Pandas and Polars.\")\r\n",
    "\r\n",
    "# Save Polars DF for persistence\r\n",
    "df_polars.write_parquet(\"all_stocks_5yr_polars.parquet\")\r\n",
    "\r\n",
    "### === Technical Indicators Calculation Functions === ###\r\n",
    "\r\n",
    "# 1. Relative Strength Index (RSI)\r\n",
    "def calculate_rsi_pandas(df, period=14):\r\n",
    "    delta = df[\"close\"].diff()\r\n",
    "    gain = np.where(delta > 0, delta, 0)\r\n",
    "    loss = np.where(delta < 0, -delta, 0)\r\n",
    "    avg_gain = pd.Series(gain).rolling(window=period, min_periods=1).mean()\r\n",
    "    avg_loss = pd.Series(loss).rolling(window=period, min_periods=1).mean()\r\n",
    "    rs = avg_gain / (avg_loss + 1e-10)\r\n",
    "    rsi = 100 - (100 / (1 + rs))\r\n",
    "    return rsi\r\n",
    "\r\n",
    "def calculate_rsi_polars(df, period=14):\r\n",
    "    df = df.with_columns((df[\"close\"].diff()).alias(\"delta\"))\r\n",
    "    gain = df.with_columns((pl.when(df[\"delta\"] > 0, df[\"delta\"]).otherwise(0)).alias(\"gain\"))\r\n",
    "    loss = df.with_columns((pl.when(df[\"delta\"] < 0, -df[\"delta\"]).otherwise(0)).alias(\"loss\"))\r\n",
    "    avg_gain = gain[\"gain\"].rolling_mean(period)\r\n",
    "    avg_loss = loss[\"loss\"].rolling_mean(period)\r\n",
    "    rs = avg_gain / (avg_loss + 1e-10)\r\n",
    "    rsi = 100 - (100 / (1 + rs))\r\n",
    "    return rsi\r\n",
    "\r\n",
    "# 2. Money Flow Index (MFI)\r\n",
    "def calculate_mfi_pandas(df, period=14):\r\n",
    "    typical_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\r\n",
    "    money_flow = typical_price * df[\"volume\"]\r\n",
    "    positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0)\r\n",
    "    negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0)\r\n",
    "    mf_ratio = positive_flow.rolling(period).sum() / (negative_flow.rolling(period).sum() + 1e-10)\r\n",
    "    mfi = 100 - (100 / (1 + mf_ratio))\r\n",
    "    return mfi\r\n",
    "\r\n",
    "def calculate_mfi_polars(df, period=14):\r\n",
    "    typical_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\r\n",
    "    money_flow = typical_price * df[\"volume\"]\r\n",
    "    positive_flow = money_flow.filter(typical_price > typical_price.shift(1))\r\n",
    "    negative_flow = money_flow.filter(typical_price < typical_price.shift(1))\r\n",
    "    mf_ratio = positive_flow.rolling_sum(period) / (negative_flow.rolling_sum(period) + 1e-10)\r\n",
    "    mfi = 100 - (100 / (1 + mf_ratio))\r\n",
    "    return mfi\r\n",
    "\r\n",
    "# 3. Stochastics\r\n",
    "def calculate_stoch_pandas(df, period=14):\r\n",
    "    lowest_low = df[\"low\"].rolling(window=period).min()\r\n",
    "    highest_high = df[\"high\"].rolling(window=period).max()\r\n",
    "    stoch = 100 * (df[\"close\"] - lowest_low) / (highest_high - lowest_low + 1e-10)\r\n",
    "    return stoch\r\n",
    "\r\n",
    "def calculate_stoch_polars(df, period=14):\r\n",
    "    lowest_low = df[\"low\"].rolling_min(period)\r\n",
    "    highest_high = df[\"high\"].rolling_max(period)\r\n",
    "    stoch = 100 * (df[\"close\"] - lowest_low) / (highest_high - lowest_low + 1e-10)\r\n",
    "    return stoch\r\n",
    "\r\n",
    "# 4. Moving Average Convergence Divergence (MACD)\r\n",
    "def calculate_macd_pandas(df, short=12, long=26, signal=9):\r\n",
    "    short_ema = df[\"close\"].ewm(span=short, adjust=False).mean()\r\n",
    "    long_ema = df[\"close\"].ewm(span=long, adjust=False).mean()\r\n",
    "    macd = short_ema - long_ema\r\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\r\n",
    "    return macd, signal_line\r\n",
    "\r\n",
    "def calculate_macd_polars(df, short=12, long=26, signal=9):\r\n",
    "    short_ema = df[\"close\"].rolling_mean(short)\r\n",
    "    long_ema = df[\"close\"].rolling_mean(long)\r\n",
    "    macd = short_ema - long_ema\r\n",
    "    signal_line = macd.rolling_mean(signal)\r\n",
    "    return macd, signal_line\r\n",
    "\r\n",
    "### === Performance Benchmark === ###\r\n",
    "results = []\r\n",
    "\r\n",
    "def benchmark(func, df, label):\r\n",
    "    \"\"\" Measure execution time of a function \"\"\"\r\n",
    "    start_time = time.time()\r\n",
    "    result = func(df)\r\n",
    "    exec_time = time.time() - start_time\r\n",
    "    return {\"Indicator\": label, \"Time (s)\": exec_time}\r\n",
    "\r\n",
    "# Run Benchmarks for Pandas\r\n",
    "print(\"\\nRunning calculations in Pandas...\")\r\n",
    "results.append(benchmark(lambda df: calculate_rsi_pandas(df), df_pandas, \"RSI - Pandas\"))\r\n",
    "results.append(benchmark(lambda df: calculate_mfi_pandas(df), df_pandas, \"MFI - Pandas\"))\r\n",
    "results.append(benchmark(lambda df: calculate_stoch_pandas(df), df_pandas, \"Stochastic - Pandas\"))\r\n",
    "results.append(benchmark(lambda df: calculate_macd_pandas(df), df_pandas, \"MACD - Pandas\"))\r\n",
    "\r\n",
    "# Run Benchmarks for Polars\r\n",
    "print(\"\\nRunning calculations in Polars...\")\r\n",
    "results.append(benchmark(lambda df: calculate_rsi_polars(df), df_polars, \"RSI - Polars\"))\r\n",
    "results.append(benchmark(lambda df: calculate_mfi_polars(df), df_polars, \"MFI - Polars\"))\r\n",
    "results.append(benchmark(lambda df: calculate_stoch_polars(df), df_polars, \"Stochastic - Polars\"))\r\n",
    "results.append(benchmark(lambda df: calculate_macd_polars(df), df_polars, \"MACD - Polars\"))\r\n",
    "\r\n",
    "# Convert results to DataFrame\r\n",
    "results_df = pd.DataFrame(results)\r\n",
    "\r\n",
    "# Create a comparison table\r\n",
    "comparison_table = results_df.pivot(index=\"Indicator\", columns=\"Indicator\", values=\"Time (s)\")\r\n",
    "\r\n",
    "# Display Results\r\n",
    "print(\"\\n=== Performance Comparison: Pandas vs. Polars ===\")\r\n",
    "display(comparison_table)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28564982-eda5-4506-8149-4a313d52cd8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df_polars \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_stocks_5yr.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m df_polars\u001b[38;5;241m.\u001b[39mhead()  \u001b[38;5;66;03m# Display first few rows\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "df_polars = pl.read_csv(\"all_stocks_5yr.csv\")\n",
    "df_polars.head()  # Display first few rows\n",
    "df_polars.describe()  # Show statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4780fe1-8127-40cd-9255-687e53162c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
