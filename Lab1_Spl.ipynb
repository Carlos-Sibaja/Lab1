{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35df0306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: polars in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (1.24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: streamlit in c:\\programdata\\anaconda3\\lib\\site-packages (1.32.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (10.3.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (2.32.2)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (13.3.5)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars\n",
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419778cd-43cd-4e67-af2e-507a614cdff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import polars as pl\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from tabulate import tabulate \n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2ca2f5-d772-406b-ad62-3c1a6c787913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 619040 entries, 0 to 619039\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    619040 non-null  object \n",
      " 1   open    619029 non-null  float64\n",
      " 2   high    619032 non-null  float64\n",
      " 3   low     619032 non-null  float64\n",
      " 4   close   619040 non-null  float64\n",
      " 5   volume  619040 non-null  int64  \n",
      " 6   name    619040 non-null  object \n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset to understand its structure\n",
    "csv_filename= pd.read_csv('all_stocks_5yr.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "csv_filename.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77092525-0494-4e29-ad6d-2ab8597fb894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CSV vs. Parquet Benchmark Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scale</th>\n",
       "      <th>CSV_Size_MB</th>\n",
       "      <th>CSV_Write_Time_s</th>\n",
       "      <th>CSV_Read_Time_s</th>\n",
       "      <th>Parquet_Size_MB</th>\n",
       "      <th>Parquet_Write_Time_s</th>\n",
       "      <th>Parquet_Read_Time_s</th>\n",
       "      <th>Compression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>28.800573</td>\n",
       "      <td>1.744498</td>\n",
       "      <td>0.290820</td>\n",
       "      <td>12.730873</td>\n",
       "      <td>0.240295</td>\n",
       "      <td>0.080239</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28.800573</td>\n",
       "      <td>1.388274</td>\n",
       "      <td>0.248281</td>\n",
       "      <td>10.151073</td>\n",
       "      <td>0.318887</td>\n",
       "      <td>0.093797</td>\n",
       "      <td>snappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>28.800573</td>\n",
       "      <td>1.372739</td>\n",
       "      <td>0.263308</td>\n",
       "      <td>8.058299</td>\n",
       "      <td>0.754728</td>\n",
       "      <td>0.090736</td>\n",
       "      <td>gzip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28.800573</td>\n",
       "      <td>1.634053</td>\n",
       "      <td>0.304246</td>\n",
       "      <td>7.755613</td>\n",
       "      <td>1.059373</td>\n",
       "      <td>0.099399</td>\n",
       "      <td>brotli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>288.005407</td>\n",
       "      <td>14.792532</td>\n",
       "      <td>2.905405</td>\n",
       "      <td>118.028409</td>\n",
       "      <td>2.591053</td>\n",
       "      <td>0.743071</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>288.005407</td>\n",
       "      <td>17.212445</td>\n",
       "      <td>3.333902</td>\n",
       "      <td>95.354448</td>\n",
       "      <td>2.991426</td>\n",
       "      <td>0.794729</td>\n",
       "      <td>snappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>288.005407</td>\n",
       "      <td>17.884880</td>\n",
       "      <td>3.291981</td>\n",
       "      <td>75.976962</td>\n",
       "      <td>10.097118</td>\n",
       "      <td>0.805354</td>\n",
       "      <td>gzip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>288.005407</td>\n",
       "      <td>18.352993</td>\n",
       "      <td>2.891467</td>\n",
       "      <td>73.180475</td>\n",
       "      <td>10.017375</td>\n",
       "      <td>0.818962</td>\n",
       "      <td>brotli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>2880.053747</td>\n",
       "      <td>201.342551</td>\n",
       "      <td>36.028684</td>\n",
       "      <td>1178.436979</td>\n",
       "      <td>33.597701</td>\n",
       "      <td>9.590052</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>2880.053747</td>\n",
       "      <td>196.529372</td>\n",
       "      <td>39.503717</td>\n",
       "      <td>951.709143</td>\n",
       "      <td>28.276961</td>\n",
       "      <td>6.639698</td>\n",
       "      <td>snappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>2880.053747</td>\n",
       "      <td>175.627810</td>\n",
       "      <td>29.070336</td>\n",
       "      <td>758.129578</td>\n",
       "      <td>86.556197</td>\n",
       "      <td>9.477390</td>\n",
       "      <td>gzip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>2880.053747</td>\n",
       "      <td>194.932373</td>\n",
       "      <td>33.856190</td>\n",
       "      <td>730.511627</td>\n",
       "      <td>102.438890</td>\n",
       "      <td>9.580216</td>\n",
       "      <td>brotli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Scale  CSV_Size_MB  CSV_Write_Time_s  CSV_Read_Time_s  Parquet_Size_MB  \\\n",
       "0       1    28.800573          1.744498         0.290820        12.730873   \n",
       "1       1    28.800573          1.388274         0.248281        10.151073   \n",
       "2       1    28.800573          1.372739         0.263308         8.058299   \n",
       "3       1    28.800573          1.634053         0.304246         7.755613   \n",
       "4      10   288.005407         14.792532         2.905405       118.028409   \n",
       "5      10   288.005407         17.212445         3.333902        95.354448   \n",
       "6      10   288.005407         17.884880         3.291981        75.976962   \n",
       "7      10   288.005407         18.352993         2.891467        73.180475   \n",
       "8     100  2880.053747        201.342551        36.028684      1178.436979   \n",
       "9     100  2880.053747        196.529372        39.503717       951.709143   \n",
       "10    100  2880.053747        175.627810        29.070336       758.129578   \n",
       "11    100  2880.053747        194.932373        33.856190       730.511627   \n",
       "\n",
       "    Parquet_Write_Time_s  Parquet_Read_Time_s Compression  \n",
       "0               0.240295             0.080239        None  \n",
       "1               0.318887             0.093797      snappy  \n",
       "2               0.754728             0.090736        gzip  \n",
       "3               1.059373             0.099399      brotli  \n",
       "4               2.591053             0.743071        None  \n",
       "5               2.991426             0.794729      snappy  \n",
       "6              10.097118             0.805354        gzip  \n",
       "7              10.017375             0.818962      brotli  \n",
       "8              33.597701             9.590052        None  \n",
       "9              28.276961             6.639698      snappy  \n",
       "10             86.556197             9.477390        gzip  \n",
       "11            102.438890             9.580216      brotli  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the original dataset\n",
    "datastocks = \"all_stocks_5yr.csv\"  \n",
    "df_original = pd.read_csv(datastocks)\n",
    "\n",
    "# Benchmark function for evaluating CSV vs. Parquet\n",
    "def benchmark(df, scale_factor, compression=None):\n",
    "    \"\"\"Evaluates CSV vs. Parquet in terms of size and speed at different scales.\"\"\"\n",
    "    scaled_df = pd.concat([df] * scale_factor, ignore_index=True)\n",
    "    csv_filename = f\"data_{scale_factor}x.csv\"\n",
    "    parquet_filename = f\"data_{scale_factor}x.parquet\"\n",
    "\n",
    "    # ---- CSV Evaluation ----\n",
    "    start_time = time.time()\n",
    "    scaled_df.to_csv(csv_filename, index=False)\n",
    "    csv_write_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    df_csv = pd.read_csv(csv_filename)\n",
    "    csv_read_time = time.time() - start_time\n",
    "\n",
    "    csv_size = os.path.getsize(csv_filename) / (1024 * 1024)  # Size in MB\n",
    "\n",
    "    # ---- Parquet Evaluation ----\n",
    "    start_time = time.time()\n",
    "    scaled_df.to_parquet(parquet_filename, index=False, compression=compression, engine=\"pyarrow\")\n",
    "    parquet_write_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    df_parquet = pd.read_parquet(parquet_filename, engine=\"pyarrow\")\n",
    "    parquet_read_time = time.time() - start_time\n",
    "\n",
    "    parquet_size = os.path.getsize(parquet_filename) / (1024 * 1024)  # Size in MB\n",
    "\n",
    "    # Remove files to save space\n",
    "    os.remove(csv_filename)\n",
    "    os.remove(parquet_filename)\n",
    "\n",
    "    # Clean Cache\n",
    "    del scaled_df, df_csv, df_parquet  # to delete the dataframes\n",
    "    gc.collect()  # to empty the cache\n",
    "\n",
    "    # Return results    \n",
    "    return {\n",
    "        \"Scale\": scale_factor,\n",
    "        \"CSV_Size_MB\": csv_size,\n",
    "        \"CSV_Write_Time_s\": csv_write_time,\n",
    "        \"CSV_Read_Time_s\": csv_read_time,\n",
    "        \"Parquet_Size_MB\": parquet_size,\n",
    "        \"Parquet_Write_Time_s\": parquet_write_time,\n",
    "        \"Parquet_Read_Time_s\": parquet_read_time,\n",
    "        \"Compression\": compression\n",
    "    }\n",
    "\n",
    "# Run benchmarks at scales 1x, 10x, 100x with different compression methods\n",
    "scales = [1, 10, 100]\n",
    "compressions = [None, \"snappy\", \"gzip\", \"brotli\"]\n",
    "results = []\n",
    "\n",
    "for scale in scales:\n",
    "    for comp in compressions:\n",
    "        results.append(benchmark(df_original, scale, comp))\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the benchmark results to a CSV file\n",
    "results_df.to_csv(\"benchmark_csv_parquet.csv\", index=False)\n",
    "\n",
    "# Print the results in the Jupyter Notebook\n",
    "print(\"\\n=== CSV vs. Parquet Benchmark Results ===\")\n",
    "display(results_df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ae483-6e23-467f-a4ee-af09789edbbd",
   "metadata": {},
   "source": [
    "## PART1 B   Perfomance Pandas Vs Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fe65d-5619-4e30-9c7d-48799ccaf1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded in both Pandas and Polars.\n",
      "\n",
      "Running calculations in Pandas...\n",
      "\n",
      "Running calculations in Polars...\n",
      "\n",
      "=== Performance Comparison: Pandas vs. Polars ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indicator</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MACD - Pandas</th>\n",
       "      <td>0.022887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACD - Polars</th>\n",
       "      <td>0.024914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFI - Pandas</th>\n",
       "      <td>0.050522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFI - Polars</th>\n",
       "      <td>0.043442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSI - Pandas</th>\n",
       "      <td>0.040365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSI - Polars</th>\n",
       "      <td>0.027666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stochastic - Pandas</th>\n",
       "      <td>0.055411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stochastic - Polars</th>\n",
       "      <td>0.031308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Time (s)\n",
       "Indicator                    \n",
       "MACD - Pandas        0.022887\n",
       "MACD - Polars        0.024914\n",
       "MFI - Pandas         0.050522\n",
       "MFI - Polars         0.043442\n",
       "RSI - Pandas         0.040365\n",
       "RSI - Polars         0.027666\n",
       "Stochastic - Pandas  0.055411\n",
       "Stochastic - Polars  0.031308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted all_stocks_5yr_polars.parquet to save space.\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset in both Pandas and Polars\n",
    "csv_file = \"all_stocks_5yr.csv\"\n",
    "\n",
    "# Load with Pandas\n",
    "df_pandas = pd.read_csv(csv_file)\n",
    "df_pandas['date'] = pd.to_datetime(df_pandas['date'])\n",
    "df_pandas = df_pandas.sort_values('date')\n",
    "\n",
    "# Load with Polars\n",
    "df_polars = pl.read_csv(csv_file)\n",
    "df_polars = df_polars.with_columns(pl.col(\"date\").str.to_date())\n",
    "\n",
    "print(\"Dataset successfully loaded in both Pandas and Polars.\")\n",
    "\n",
    "# Save Polars DF for persistence\n",
    "df_polars.write_parquet(\"all_stocks_5yr_polars.parquet\")\n",
    "\n",
    "### === Technical Indicators Calculation Functions === ###\n",
    "\n",
    "# 1. Relative Strength Index (RSI)\n",
    "def calculate_rsi_pandas(df, period=14):\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    loss = np.where(delta < 0, -delta, 0)\n",
    "    avg_gain = pd.Series(gain).rolling(window=period, min_periods=1).mean()\n",
    "    avg_loss = pd.Series(loss).rolling(window=period, min_periods=1).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-10)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_rsi_polars(df, period=14):\n",
    "    df = df.with_columns((df[\"close\"].diff()).alias(\"delta\"))\n",
    "    gain = df.with_columns(pl.when(df[\"delta\"] > 0).then(df[\"delta\"]).otherwise(0).alias(\"gain\"))\n",
    "    loss = df.with_columns(pl.when(df[\"delta\"] < 0).then(-df[\"delta\"]).otherwise(0).alias(\"loss\"))\n",
    "    avg_gain = gain[\"gain\"].rolling_mean(period)\n",
    "    avg_loss = loss[\"loss\"].rolling_mean(period)\n",
    "    rs = avg_gain / (avg_loss + 1e-10)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# 2. Money Flow Index (MFI)\n",
    "def calculate_mfi_pandas(df, period=14):\n",
    "    typical_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "    money_flow = typical_price * df[\"volume\"]\n",
    "    positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0)\n",
    "    negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0)\n",
    "    mf_ratio = positive_flow.rolling(period).sum() / (negative_flow.rolling(period).sum() + 1e-10)\n",
    "    mfi = 100 - (100 / (1 + mf_ratio))\n",
    "    return mfi\n",
    "\n",
    "def calculate_mfi_polars(df, period=14):\n",
    "    typical_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "    money_flow = typical_price * df[\"volume\"]\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        (typical_price > typical_price.shift(1)).alias(\"positive_mask\"),\n",
    "        (typical_price < typical_price.shift(1)).alias(\"negative_mask\")\n",
    "    )\n",
    "    \n",
    "    positive_flow = money_flow * df[\"positive_mask\"]\n",
    "    negative_flow = money_flow * df[\"negative_mask\"]\n",
    "    \n",
    "    positive_flow_sum = positive_flow.rolling_sum(period)\n",
    "    negative_flow_sum = negative_flow.rolling_sum(period)\n",
    "    \n",
    "    mf_ratio = positive_flow_sum / (negative_flow_sum + 1e-10)\n",
    "    mfi = 100 - (100 / (1 + mf_ratio))\n",
    "    \n",
    "    return mfi\n",
    "\n",
    "# 3. Stochastics\n",
    "def calculate_stoch_pandas(df, period=14):\n",
    "    lowest_low = df[\"low\"].rolling(window=period).min()\n",
    "    highest_high = df[\"high\"].rolling(window=period).max()\n",
    "    stoch = 100 * (df[\"close\"] - lowest_low) / (highest_high - lowest_low + 1e-10)\n",
    "    return stoch\n",
    "\n",
    "def calculate_stoch_polars(df, period=14):\n",
    "    lowest_low = df[\"low\"].rolling_min(period)\n",
    "    highest_high = df[\"high\"].rolling_max(period)\n",
    "    stoch = 100 * (df[\"close\"] - lowest_low) / (highest_high - lowest_low + 1e-10)\n",
    "    return stoch\n",
    "\n",
    "# 4. Moving Average Convergence Divergence (MACD)\n",
    "def calculate_macd_pandas(df, short=12, long=26, signal=9):\n",
    "    short_ema = df[\"close\"].ewm(span=short, adjust=False).mean()\n",
    "    long_ema = df[\"close\"].ewm(span=long, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    return macd, signal_line\n",
    "\n",
    "def calculate_macd_polars(df, short=12, long=26, signal=9):\n",
    "    short_ema = df[\"close\"].rolling_mean(short)\n",
    "    long_ema = df[\"close\"].rolling_mean(long)\n",
    "    macd = short_ema - long_ema\n",
    "    signal_line = macd.rolling_mean(signal)\n",
    "    return macd, signal_line\n",
    "\n",
    "### === Performance Benchmark === ###\n",
    "results = []\n",
    "\n",
    "def benchmark(func, df, label):\n",
    "    \"\"\" Measure execution time of a function \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(df)\n",
    "    exec_time = time.time() - start_time\n",
    "    return {\"Indicator\": label, \"Time (s)\": exec_time}\n",
    "\n",
    "# Run Benchmarks for Pandas\n",
    "print(\"\\nRunning calculations in Pandas...\")\n",
    "results.append(benchmark(lambda df: calculate_rsi_pandas(df), df_pandas, \"RSI - Pandas\"))\n",
    "results.append(benchmark(lambda df: calculate_mfi_pandas(df), df_pandas, \"MFI - Pandas\"))\n",
    "results.append(benchmark(lambda df: calculate_stoch_pandas(df), df_pandas, \"Stochastic - Pandas\"))\n",
    "results.append(benchmark(lambda df: calculate_macd_pandas(df), df_pandas, \"MACD - Pandas\"))\n",
    "\n",
    "# Run Benchmarks for Polars\n",
    "print(\"\\nRunning calculations in Polars...\")\n",
    "results.append(benchmark(lambda df: calculate_rsi_polars(df), df_polars, \"RSI - Polars\"))\n",
    "results.append(benchmark(lambda df: calculate_mfi_polars(df), df_polars, \"MFI - Polars\"))\n",
    "results.append(benchmark(lambda df: calculate_stoch_polars(df), df_polars, \"Stochastic - Polars\"))\n",
    "results.append(benchmark(lambda df: calculate_macd_polars(df), df_polars, \"MACD - Polars\"))\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the benchmark results to a CSV file\n",
    "results_df.to_csv(\"Benchmark_Pandas_Polars.csv\", index=False)\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_table = results_df.pivot_table(index=\"Indicator\", values=\"Time (s)\")\n",
    "\n",
    "# Display Results\n",
    "print(\"\\n=== Performance Comparison: Pandas vs. Polars ===\")\n",
    "display(comparison_table)\n",
    "\n",
    "import os\n",
    "\n",
    "# Remove files to save space\n",
    "try:\n",
    "    if os.path.exists(\"all_stocks_5yr_polars.parquet\"):\n",
    "        os.remove(\"all_stocks_5yr_polars.parquet\")\n",
    "        print(f\"Deleted all_stocks_5yr_polars.parquet to save space.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting all_stocks_5yr_polars.parquet: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698cd0dd",
   "metadata": {},
   "source": [
    "# PART 2 Prediction Algoritms\n",
    "* Using Linear Regression\n",
    "* Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07ffb241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Predicted Next Day Closing Price ===\n",
      "+-----------+---------------------+----------------------+--------------------------------+----------------------------+\n",
      "| Company   | Last Date           |   Last Closing Price |   Linear Regression Prediction |   Random Forest Prediction |\n",
      "+===========+=====================+======================+================================+============================+\n",
      "| AAPL      | 2018-02-07 00:00:00 |               159.54 |                        163.029 |                    161.516 |\n",
      "+-----------+---------------------+----------------------+--------------------------------+----------------------------+\n",
      "| GOOGL     | 2018-02-07 00:00:00 |              1055.41 |                       1081.65  |                   1073.54  |\n",
      "+-----------+---------------------+----------------------+--------------------------------+----------------------------+\n",
      "| AMZN      | 2018-02-07 00:00:00 |              1416.78 |                       1444.31  |                   1409.55  |\n",
      "+-----------+---------------------+----------------------+--------------------------------+----------------------------+\n",
      "\n",
      "=== R² Score Comparison ===\n",
      "+-----------+------------------------+--------------------+\n",
      "| Company   |   R² Linear Regression |   R² Random Forest |\n",
      "+===========+========================+====================+\n",
      "| AAPL      |               0.997068 |           0.996239 |\n",
      "+-----------+------------------------+--------------------+\n",
      "| GOOGL     |               0.996878 |           0.99664  |\n",
      "+-----------+------------------------+--------------------+\n",
      "| AMZN      |               0.998713 |           0.998552 |\n",
      "+-----------+------------------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from tabulate import tabulate  # To display results in table format\n",
    "\n",
    "# Load dataset\n",
    "csv_file = \"all_stocks_5yr.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert date column to datetime and sort values\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(\"date\")\n",
    "\n",
    "# Select three companies\n",
    "selected_companies = [\"AAPL\", \"GOOGL\", \"AMZN\"]  # Example companies\n",
    "df_selected = df.loc[df[\"name\"].isin(selected_companies)]\n",
    "\n",
    "# Initialize storage for results\n",
    "results = []\n",
    "r2_scores = []\n",
    "\n",
    "# Process each company separately\n",
    "for company in selected_companies:\n",
    "    df_company = df_selected[df_selected[\"name\"] == company].copy()\n",
    "\n",
    "    # Create additional features\n",
    "    df_company[\"SMA_10\"] = df_company[\"close\"].rolling(window=10).mean()  # 10-day moving average\n",
    "    df_company[\"Volatility\"] = df_company[\"close\"].rolling(window=10).std()  # Standard deviation (volatility)\n",
    "\n",
    "    # Drop NaN values from rolling calculations\n",
    "    df_company = df_company.dropna()\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    X = df_company[[\"close\", \"volume\", \"SMA_10\", \"Volatility\"]]\n",
    "    y = df_company[\"close\"].shift(-1)  # Predict next day's closing price\n",
    "\n",
    "    # Drop last row because it has NaN in y (shifted row)\n",
    "    X, y = X[:-1], y[:-1]\n",
    "\n",
    "    # Split into 80% training and 20% testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train Linear Regression model\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    lin_reg_pred = lin_reg.predict(X_test)\n",
    "    lin_reg_r2 = r2_score(y_test, lin_reg_pred)\n",
    "\n",
    "    # Train Random Forest model\n",
    "    rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_reg.fit(X_train, y_train)\n",
    "    rf_reg_pred = rf_reg.predict(X_test)\n",
    "    rf_reg_r2 = r2_score(y_test, rf_reg_pred)\n",
    "\n",
    "    # Get the last available row for prediction\n",
    "    last_row = X.iloc[-1].values.reshape(1, -1)\n",
    "    predicted_lin = lin_reg.predict(last_row)[0]\n",
    "    predicted_rf = rf_reg.predict(last_row)[0]\n",
    "\n",
    "    # Store results\n",
    "    last_date = df_company[\"date\"].iloc[-1]\n",
    "    actual_last_price = df_company[\"close\"].iloc[-1]\n",
    "    results.append([company, last_date, actual_last_price, predicted_lin, predicted_rf])\n",
    "    r2_scores.append([company, lin_reg_r2, rf_reg_r2])\n",
    "\n",
    "# Create table for predictions\n",
    "print(\"\\n=== Predicted Next Day Closing Price ===\")\n",
    "print(tabulate(results, headers=[\"Company\", \"Last Date\", \"Last Closing Price\", \"Linear Regression Prediction\", \"Random Forest Prediction\"], tablefmt=\"grid\"))\n",
    "\n",
    "# Create table for R² comparison\n",
    "print(\"\\n=== R² Score Comparison ===\")\n",
    "print(tabulate(r2_scores, headers=[\"Company\", \"R² Linear Regression\", \"R² Random Forest\"], tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
